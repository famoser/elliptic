{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Ye4wMtzYAkqC",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "# Hypothesis testing for leakage assessment of timing side channel"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Collect the measurements\n",
    "\n",
    "First, we need to collect as clean measurements as possible.\n",
    "\n",
    "Prepare your system:\n",
    "- Disable turboboost (check `intel_pstate` is the active driver with `cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_driver`; then execute `echo \"1\" | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo`; revert after measurement)\n",
    "- Fix the frequency of some core using `cpupower frequency-set -d 1900Mhz`\n",
    "- Set `echo 1000000 > /proc/sys/kernel/sched_rt_runtime_us` to enable real-time process to use 100% of CPU (no scheduling)\n",
    "- Start the process with `taskset --cpu-list 2 chrt -f 99 php tests/consttime/collect.php` (run on core 0 or 1 with high priority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1G2zkbhcAkqE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Assess measurement quality\n",
    "\n",
    "First, we assess the measurement quality. If the measurement quality is poor, the results are irrelevant."
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1751205924984,
     "user": {
      "displayName": "Senna",
      "userId": "14138385601242009551"
     },
     "user_tz": -120
    },
    "id": "hdXdx62rAkqD",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 46,
     "status": "error",
     "timestamp": 1751205925023,
     "user": {
      "displayName": "Senna",
      "userId": "14138385601242009551"
     },
     "user_tz": -120
    },
    "id": "7ZOG0xpRAkqD",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "bffbf937-5167-4736-a2fc-0291fe4cd5c0"
   },
   "cell_type": "code",
   "source": [
    "def load_json_from_file(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "data = load_json_from_file('measurements/EDMath_ed448_400.json')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 The iteration must be irrelevant\n",
    "\n",
    "Measurement proceeds in iterations, and in each iteration all testcases are executed and measured. Hence the timing for each iteration should be the same.\n",
    "\n",
    "On a real machine, reaching this is hard, even when configuring it properly (see chapter 1). Hence we first remove iterations that are slower than the rest. You can adjust the behaviour with the following parameter: `iteration_selectivity` defines how many iterations are kept, i.e. only one out of `iteration_selectivity` iterations."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_iteration(samples):\n",
    "    measurements_per_iteration = {}\n",
    "    for iteration in range(0, len(samples[0]['measurements'])):\n",
    "        measurements_per_iteration[iteration] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        for iteration, measurement in enumerate(sample['measurements']):\n",
    "            measurements_per_iteration[iteration].append(measurement['time'])\n",
    "\n",
    "    return measurements_per_iteration\n",
    "\n",
    "measurements_per_iteration = get_measurements_per_iteration(data['samples'])\n",
    "measurements_per_iteration_sum = [__builtins__.sum(measurements_per_iteration[i]) for i in measurements_per_iteration]\n",
    "\n",
    "iteration_selectivity = 2\n",
    "threshold = sorted(measurements_per_iteration_sum)[int(len(measurements_per_iteration_sum)/iteration_selectivity)]\n",
    "remove_iterations = []\n",
    "for index, sum in enumerate(measurements_per_iteration_sum):\n",
    "    if sum > threshold:\n",
    "        remove_iterations.append(index)\n",
    "\n",
    "cleaned_samples = []\n",
    "for sample in data['samples']:\n",
    "    cleaned_measurements = []\n",
    "    for index, measurement in enumerate(sample['measurements']):\n",
    "        if index not in remove_iterations:\n",
    "            cleaned_measurements.append(measurement)\n",
    "    cleaned_sample = sample.copy()\n",
    "    cleaned_sample['measurements'] = cleaned_measurements\n",
    "    cleaned_samples.append(cleaned_sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Check the graph.** If the measurement is accurate, then for each iteration, the timings are the same."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "measurements_per_iteration = get_measurements_per_iteration(cleaned_samples)\n",
    "measurements_per_iteration_sum = [__builtins__.sum(measurements_per_iteration[i]) for i in measurements_per_iteration]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(measurements_per_iteration_sum, label='sum of measurement per iteration')\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Testcase index must be irrelevant\n",
    "\n",
    "To avoid side-effects in between the testcases (e.g. by the branch predictor or other forms of caching), we randomize the order of the testcases for each iteration. If the randomization is non-biased, and caching is successfully made irrelevant, then for each testcase index the measurement should be the same.\n",
    "\n",
    "**Check the graph.** If the measurement is accurate, then for each test index, the timings are the same."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_index(samples):\n",
    "    measurements_per_index = {}\n",
    "    for index in range(0, len(samples)):\n",
    "        measurements_per_index[index] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements_per_index[measurement['index']].append(measurement['time'])\n",
    "\n",
    "    return measurements_per_index\n",
    "\n",
    "measurements_per_index = get_measurements_per_index(cleaned_samples)\n",
    "measurements_per_index_sum = [__builtins__.sum(measurements_per_index[i]) for i in measurements_per_index]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(measurements_per_index_sum, label='sum of measurement per index')\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Assess measurement result\n",
    "\n",
    "If chapter 2 does not show signs of a botched measurement, we can now proceed to assess the measurement result."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1. Measurements per flag\n",
    "\n",
    "We quantify by flag. Flags usually indicate the type of the testcase, possibly special chosen (e.g. to multiply by 0 or similar). Note that not all testsets have a lot (of useful) flags."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_flag(samples):\n",
    "    measurements_per_flag = {'Basic': []}\n",
    "    for sample in samples:\n",
    "        for flag in sample['flags']:\n",
    "            if flag not in measurements_per_flag:\n",
    "                measurements_per_flag[flag] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        measurements = []\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "        if len(sample['flags']) == 0:\n",
    "            measurements_per_flag['Basic'].extend(measurements)\n",
    "\n",
    "        for flag in sample['flags']:\n",
    "                measurements_per_flag[flag].extend(measurements)\n",
    "\n",
    "    return measurements_per_flag"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "id": "6sEEKDtMAkqE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ee0b0306-b3c8-428f-e4d7-debeec70a158"
   },
   "source": [
    "measurements_per_flag = get_measurements_per_flag(cleaned_samples)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for flag in measurements_per_flag:\n",
    "  plt.plot(measurements_per_flag[flag], label=flag)\n",
    "plt.title(data['curve'] + ' using math ' + data['math'] + ' per flag')\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.violinplot(measurements_per_flag.values())\n",
    "plt.xticks(list(range(1, len(measurements_per_flag.keys()) + 1)), measurements_per_flag.keys())\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.ylabel('time')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for flag in measurements_per_flag:\n",
    "    # equal_var=False ensures we do not assume equal variance between the two testsets\n",
    "    result=ss.ttest_ind(measurements_per_flag['Basic'], measurements_per_flag[flag], axis=0, equal_var=False)\n",
    "    print(f\"P-value {flag}: {result.pvalue}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2. Measurement per sample\n",
    "\n",
    "We quantify by testcase.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_sample(samples):\n",
    "    measurements_per_sample = {}\n",
    "    for sample in samples:\n",
    "        measurements = []\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "        measurements_per_sample[sample['id']] = measurements\n",
    "\n",
    "    return measurements_per_sample\n",
    "\n",
    "def get_all_measurements(samples):\n",
    "    measurements = []\n",
    "    for sample in samples:\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "    return measurements\n",
    "\n",
    "measurements_per_sample = get_measurements_per_sample(cleaned_samples)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for sample in measurements_per_sample:\n",
    "  plt.plot(measurements_per_sample[sample], label=sample)\n",
    "plt.title(data['curve'] + ' using math ' + data['math'] + ' per sample')\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.violinplot(measurements_per_sample.values())\n",
    "plt.xticks(list(range(1, len(measurements_per_sample.keys()) + 1)), measurements_per_sample.keys())\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.ylabel('time')\n",
    "plt.show()\n",
    "\n",
    "def average(values):\n",
    "    return __builtins__.sum(values) / len(values)\n",
    "\n",
    "def variance(values):\n",
    "    avg = average(values)\n",
    "    return __builtins__.sum((x - avg) ** 2 for x in values) / len(values)\n",
    "\n",
    "baseline = get_all_measurements(cleaned_samples)\n",
    "print(f\"baseline:\\t avg {average(baseline)}\\t var {variance(baseline)}\")\n",
    "for sample in measurements_per_sample:\n",
    "    # equal_var=False ensures we do not assume equal variance between the two testsets\n",
    "    result=ss.ttest_ind(baseline, measurements_per_sample[sample], axis=0, equal_var=False)\n",
    "\n",
    "    print(f\"{sample}:\\t avg {average(measurements_per_sample[sample])}\\t var {variance(measurements_per_sample[sample])}\\t p={result.pvalue}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

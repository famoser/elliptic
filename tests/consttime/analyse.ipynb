{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Ye4wMtzYAkqC",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "# Hypothesis testing for leakage assessment of timing side channel"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Collect the measurements\n",
    "\n",
    "First, we need to collect as clean measurements as possible.\n",
    "\n",
    "Prepare your system:\n",
    "- Disable turboboost (check `intel_pstate` is the active driver with `cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_driver`; then execute `echo \"1\" | sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo`; revert after measurement)\n",
    "- Fix the frequency of some core using `cpupower frequency-set -d 1900Mhz`\n",
    "- Set `echo 1000000 > /proc/sys/kernel/sched_rt_runtime_us` to enable real-time process to use 100% of CPU (no scheduling)\n",
    "- Start the process with `taskset --cpu-list 2 chrt -f 99 php tests/consttime/bin.php` (run on core 2 as a real-time process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1G2zkbhcAkqE",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. Assess measurement quality\n",
    "\n",
    "First, we assess the measurement quality. If the measurement quality is poor, the results are irrelevant."
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1751205924984,
     "user": {
      "displayName": "Senna",
      "userId": "14138385601242009551"
     },
     "user_tz": -120
    },
    "id": "hdXdx62rAkqD",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import math as mathlib\n",
    "import os\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "collapsed": false,
    "executionInfo": {
     "elapsed": 46,
     "status": "error",
     "timestamp": 1751205925023,
     "user": {
      "displayName": "Senna",
      "userId": "14138385601242009551"
     },
     "user_tz": -120
    },
    "id": "7ZOG0xpRAkqD",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "bffbf937-5167-4736-a2fc-0291fe4cd5c0"
   },
   "cell_type": "code",
   "source": [
    "def load_json_from_file(file_path):\n",
    "  with open(file_path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "# data = load_json_from_file('measurements/SW_QT_ANeg3_Math_brainpoolP256r1_200.json')\n",
    "\n",
    "all_data = [\n",
    "    load_json_from_file('measurements/PhpeccMath_brainpool256r1_69.json'),\n",
    "    load_json_from_file('measurements/PhpeccMath_brainpool256r1_76.json'),\n",
    "    load_json_from_file('measurements/PhpeccMath_brainpool256r1_85.json'),\n",
    "]\n",
    "data = all_data[0]\n",
    "for index, more_data in enumerate(all_data):\n",
    "    for sample_index, sample in enumerate(more_data['samples']):\n",
    "        data['samples'][sample_index]['measurements'].extend(sample['measurements'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 The iteration must be irrelevant\n",
    "\n",
    "Measurement proceeds in iterations, and in each iteration all testcases are executed and measured. Hence the timing for each iteration should be the same.\n",
    "\n",
    "On a real machine, reaching this is hard, even when configuring it properly (see chapter 1). Hence we first remove iterations that are slower than the rest. You can adjust the behaviour with the following parameter: `iteration_selectivity` defines how many iterations are kept, i.e. only one out of `iteration_selectivity` iterations."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_iteration(samples):\n",
    "    measurements_per_iteration = {}\n",
    "    for iteration in range(0, len(samples[0]['measurements'])):\n",
    "        measurements_per_iteration[iteration] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        for iteration, measurement in enumerate(sample['measurements']):\n",
    "            measurements_per_iteration[iteration].append(measurement['time'])\n",
    "\n",
    "    return measurements_per_iteration\n",
    "\n",
    "measurements_per_iteration = get_measurements_per_iteration(data['samples'])\n",
    "measurements_per_iteration_sum = [__builtins__.sum(measurements_per_iteration[i]) for i in measurements_per_iteration]\n",
    "\n",
    "iteration_selectivity = 2\n",
    "threshold = sorted(measurements_per_iteration_sum)[int(len(measurements_per_iteration_sum)/iteration_selectivity)]\n",
    "remove_iterations = []\n",
    "for index, sum in enumerate(measurements_per_iteration_sum):\n",
    "    if sum > threshold:\n",
    "        remove_iterations.append(index)\n",
    "\n",
    "cleaned_samples = []\n",
    "for sample in data['samples']:\n",
    "    cleaned_measurements = []\n",
    "    for index, measurement in enumerate(sample['measurements']):\n",
    "        if index not in remove_iterations:\n",
    "            cleaned_measurements.append(measurement)\n",
    "    cleaned_sample = sample.copy()\n",
    "    cleaned_sample['measurements'] = cleaned_measurements\n",
    "    cleaned_samples.append(cleaned_sample)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Check the graph.** If the measurement is accurate, then for each iteration, the timings are the same. Concretely, you should see a graph that is essentially just noise."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "measurements_per_iteration = get_measurements_per_iteration(cleaned_samples)\n",
    "measurements_per_iteration_sum = [__builtins__.sum(measurements_per_iteration[i]) for i in measurements_per_iteration]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(measurements_per_iteration_sum, label='sum of measurement per iteration')\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Testcase index must be irrelevant\n",
    "\n",
    "To avoid side-effects in between the testcases (e.g. by the branch predictor or other forms of caching), we randomize the order of the testcases for each iteration. If the randomization is non-biased, and caching is successfully made irrelevant, then for each testcase index the measurement should be the same.\n",
    "\n",
    "**Check the graph.** If the measurement is accurate, then for each iteration, the timings are the same. Concretely, you should see a graph that is essentially just noise."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_index(samples):\n",
    "    measurements_per_index = {}\n",
    "    for index in range(0, len(samples)):\n",
    "        measurements_per_index[index] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements_per_index[measurement['index']].append(measurement['time'])\n",
    "\n",
    "    return measurements_per_index\n",
    "\n",
    "measurements_per_index = get_measurements_per_index(cleaned_samples)\n",
    "measurements_per_index_sum = [__builtins__.sum(measurements_per_index[i]) for i in measurements_per_index]\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(measurements_per_index_sum, label='sum of measurement per index')\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Assess const-time\n",
    "\n",
    "If chapter 2 does not show signs of a botched measurement, we can now proceed to assess the measurement result."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import scipy.stats as ss",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.1. Measurements per flag\n",
    "\n",
    "We quantify by flag. Flags usually indicate the type of the testcase, possibly special chosen (e.g. to multiply by 0 or similar). Note that not all testsets have a lot (of useful) flags.\n",
    "\n",
    "**Check the graphs.** If the implementation is const-time, then all test-cases should have similar timing. You should not see any \"steps\", and no trends based on the flags (i.e. testcases attributed to some flag consistently perform faster than the rest).\n",
    "\n",
    "**Check the p-values.** If the visual assessment looks good, check that also the p-values are high (i.e. > 0.05)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_flag(samples):\n",
    "    measurements_per_flag = {'Basic': []}\n",
    "    for sample in samples:\n",
    "        for flag in sample['flags']:\n",
    "            if flag not in measurements_per_flag:\n",
    "                measurements_per_flag[flag] = []\n",
    "\n",
    "    for sample in samples:\n",
    "        measurements = []\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "        if len(sample['flags']) == 0:\n",
    "            measurements_per_flag['Basic'].extend(measurements)\n",
    "\n",
    "        for flag in sample['flags']:\n",
    "                measurements_per_flag[flag].extend(measurements)\n",
    "\n",
    "    return measurements_per_flag"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "id": "6sEEKDtMAkqE",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ee0b0306-b3c8-428f-e4d7-debeec70a158"
   },
   "source": [
    "measurements_per_flag = get_measurements_per_flag(cleaned_samples)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for flag in measurements_per_flag:\n",
    "  plt.plot(measurements_per_flag[flag], label=flag)\n",
    "plt.title(data['curve'] + ' using math ' + data['math'] + ' per flag')\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.violinplot(measurements_per_flag.values())\n",
    "plt.xticks(list(range(1, len(measurements_per_flag.keys()) + 1)), measurements_per_flag.keys())\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.ylabel('time')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for flag in measurements_per_flag:\n",
    "    # equal_var=False ensures we do not assume equal variance between the two testsets\n",
    "    result=ss.ttest_ind(measurements_per_flag['Basic'], measurements_per_flag[flag], axis=0, equal_var=False)\n",
    "    print(f\"P-value {flag}: {result.pvalue}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2. Measurement per sample\n",
    "\n",
    "We quantify by testcase.\n",
    "\n",
    "**Check the graphs.** If the implementation is const-time, then all test-cases should have similar timing. You should not see different vertical groups of test-cases.\n",
    "\n",
    "**Check the p-values.** If the visual assessment looks good, check that also the p-values are high (i.e. > 0.05)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_measurements_per_sample(samples):\n",
    "    measurements_per_sample = {}\n",
    "    for sample in samples:\n",
    "        measurements = []\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "        measurements_per_sample[sample['id']] = measurements\n",
    "\n",
    "    return measurements_per_sample\n",
    "\n",
    "def get_all_measurements(samples):\n",
    "    measurements = []\n",
    "    for sample in samples:\n",
    "        for measurement in sample['measurements']:\n",
    "            measurements.append(measurement['time'])\n",
    "\n",
    "    return measurements\n",
    "\n",
    "measurements_per_sample = get_measurements_per_sample(cleaned_samples)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for sample in measurements_per_sample:\n",
    "  plt.plot(measurements_per_sample[sample], label=sample)\n",
    "plt.title(data['curve'] + ' using math ' + data['math'] + ' per sample')\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend(ncol=3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.violinplot(measurements_per_sample.values())\n",
    "plt.xticks(list(range(1, len(measurements_per_sample.keys()) + 1)), measurements_per_sample.keys())\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.ylabel('time')\n",
    "plt.show()\n",
    "\n",
    "def average(values):\n",
    "    return __builtins__.sum(values) / len(values)\n",
    "\n",
    "def variance(values):\n",
    "    avg = average(values)\n",
    "    return __builtins__.sum((x - avg) ** 2 for x in values) / len(values)\n",
    "\n",
    "baseline = get_all_measurements(cleaned_samples)\n",
    "print(f\"baseline:\\t avg {average(baseline)}\\t var {variance(baseline)}\")\n",
    "for sample in measurements_per_sample:\n",
    "    # equal_var=False ensures we do not assume equal variance between the two testsets\n",
    "    result=ss.ttest_ind(baseline, measurements_per_sample[sample], axis=0, equal_var=False)\n",
    "\n",
    "    print(f\"{sample}:\\t avg {average(measurements_per_sample[sample])}\\t var {variance(measurements_per_sample[sample])}\\t p={result.pvalue}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4. Assess impact\n",
    "\n",
    "If chapter 3 shows some evidence of non-const time behaviour, we assess here the impact."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.cluster.vq import kmeans, vq\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.1 Cluster non-const time behaviour\n",
    "\n",
    "We now separate the samples into the baseline (i.e. operating on essentially random data) from the other samples that are showing non-const time behaviour.\n",
    "\n",
    "**Decide number of clusters.** Based on the graphs in chapter 3, define the number of clusters using `expected_clusters` that need to be separated from each other.\n",
    "\n",
    "**Check graph that clusters are well separated.** Check the graph to be sure that the clusers have been properly identified. Else, adjust the clusters manually."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_average_per_sample(measurements_per_sample):\n",
    "    result = {}\n",
    "    for sample in measurements_per_sample:\n",
    "        result[sample] = average(measurements_per_sample[sample])\n",
    "\n",
    "    return result\n",
    "\n",
    "def cluster_averages(averages_per_sample, clusers_count):\n",
    "    # Calculate averages for each array\n",
    "    names = list(averages_per_sample.keys())\n",
    "    averages = np.array([arr for arr in averages_per_sample.values()])\n",
    "\n",
    "    # Reshape averages for clustering\n",
    "    data = averages.reshape(-1, 1)\n",
    "\n",
    "    # Perform k-means clustering\n",
    "    centroids, _ = kmeans(data, clusers_count, iter=1000, rng=42)\n",
    "\n",
    "    # Assign points to clusters\n",
    "    labels, _ = vq(data, centroids)\n",
    "\n",
    "    # Create groups dictionary\n",
    "    groups = {i: [] for i in range(clusers_count)}\n",
    "    group_averages = {i: [] for i in range(clusers_count)}\n",
    "\n",
    "    # Assign arrays to groups\n",
    "    for name, avg, label in zip(names, averages, labels):\n",
    "        groups[label].append(name)\n",
    "        group_averages[label].append(avg)\n",
    "\n",
    "    # Create final sorted groups\n",
    "    sorted_groups = {}\n",
    "    for i in range(clusers_count):\n",
    "        mean_of_group = np.mean(group_averages[i])\n",
    "        sorted_groups[i] = {\n",
    "            'members': groups[i],\n",
    "            'mean': mean_of_group,\n",
    "            'std': np.std(group_averages[i])\n",
    "        }\n",
    "\n",
    "    # Sort groups by mean value\n",
    "    return dict(sorted(sorted_groups.items(), key=lambda x: x[1]['mean']))\n",
    "\n",
    "expected_clusters = 6\n",
    "average_per_sample = get_average_per_sample(measurements_per_sample)\n",
    "clusters = cluster_averages(average_per_sample, expected_clusters)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "for index, cluster in enumerate(clusters):\n",
    "    for sample in clusters[cluster]['members']:\n",
    "      plt.plot(measurements_per_sample[sample], label=sample, color=colors[index % len(colors)])\n",
    "plt.title(data['curve'] + ' using math ' + data['math'] + ' per sample')\n",
    "plt.xlabel('sample')\n",
    "plt.ylabel('time')\n",
    "plt.legend(ncol=3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "parts = plt.violinplot(measurements_per_sample.values())\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    sample_name = list(measurements_per_sample.keys())[i]\n",
    "    for cluster_idx, cluster_info in clusters.items():\n",
    "        if sample_name in cluster_info['members']:\n",
    "            pc.set_facecolor(colors[cluster_idx % len(colors)])\n",
    "            pc.set_edgecolor(colors[cluster_idx % len(colors)])\n",
    "\n",
    "            break\n",
    "plt.xticks(list(range(1, len(measurements_per_sample.keys()) + 1)), measurements_per_sample.keys())\n",
    "plt.title(data['curve'] + ' using math ' + data['math'])\n",
    "plt.ylabel('time')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "columns = 2\n",
    "rows = mathlib.ceil(len(clusters) / columns)\n",
    "fig, axes = plt.subplots(rows, columns, figsize=(14, 5*rows))\n",
    "\n",
    "for row in range(0, rows):\n",
    "    for column in range(0, columns):\n",
    "        axe = axes[row][column] if rows > 1 else axes[column]\n",
    "        cluster = row * columns + column\n",
    "        if cluster >= len(clusters):\n",
    "            fig.delaxes(axe)\n",
    "            continue\n",
    "\n",
    "        for sample in clusters[cluster]['members']:\n",
    "            axe.plot(measurements_per_sample[sample], label=sample)\n",
    "\n",
    "        axe.set_xlabel('measurement')\n",
    "        axe.set_ylabel('time')\n",
    "        axe.set_title('Cluster: ' + str(cluster))\n",
    "        axe.legend(ncol=2)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
